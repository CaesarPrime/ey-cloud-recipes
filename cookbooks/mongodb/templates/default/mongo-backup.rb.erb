#!/usr/local/ey_resin/ruby/bin/ruby

gem 'fog-aws', gem '>= 0.3.0'
require 'fog/aws'
require 'rubygems'
require 'date'
require 'digest'
require 'net/http'
require 'fileutils'
require 'json'

# Human Readable Filesizes
class Integer
  def to_filesize
    {
      'B'  => 1024,
      'KB' => 1024 * 1024,
      'MB' => 1024 * 1024 * 1024,
      'GB' => 1024 * 1024 * 1024 * 1024,
      'TB' => 1024 * 1024 * 1024 * 1024 * 1024
    }.each_pair { |e, s| return "#{(self.to_f / (s / 1024)).round(2)}#{e}" if self < s }
  end
end

key = '<%= @id_key %>'
secret = '<% @secret_key %>'
region = '<% @s3_region %>'

conn = Fog::Storage.new(
  :provider => 'AWS',
  :aws_access_key_id => key,
  :aws_secret_access_key => secret,
  :region => region,
  :aws_signature_version => 2
)

@databases = JSON.parse(`mongo --quiet --eval "printjson(db.runCommand('listDatabases').databases)" admin`)
@databases.map! { |db| db["sizeOnDisk"] >= 1 ? db["name"] : nil }.compact!

@ismaster = `mongo --quiet --eval 'printjson(db.runCommand("ismaster"))' | grep ismaster | awk '{print $3}'| sed s/,//`.strip
@environment = '<%= @env %>'
@app_name = '<%= @app_name %>'
@keep = 10 * @databases.length
@bucket = "ey-backup-#{Digest::SHA1.hexdigest(key)[0..11]}-mongo-rs"
@tmpname = "#{Time.now.strftime("%Y-%m-%dT%H:%M:%S").gsub(/:/, '-')}"
FileUtils.mkdir_p '/mnt/tmp'

begin
  aws_directory = conn.directories.create( :key => @bucket, :public => false )
rescue Excon::Errors::Conflict
rescue => e
  puts e
  puts e.inspect
  exit
end

backup_success = false

if @ismaster == "false" #only dump non-primary nodes
  @databases.each do |database|
    next if database == 'local'
    token = "#{database}"
    
    object_to_upload = "/mnt/tmp/#{@app_name}-#{token}.#{@tmpname}.bson.tar.gz"
    object_key = "#{@environment}.#{@app_name}/#{@app_name}-#{token}.#{@tmpname}.bson.tar.gz"
    workdir = "/mnt/tmp/#{File.basename(object_to_upload, ".bson.tar.gz")}"
    
    mongocmd = "mongodump -h 127.0.0.1 -d #{database} -o #{workdir} && tar cjf \"#{object_to_upload}\" \"#{workdir}\""
    if system(mongocmd)
      FileUtils.rm_rf "#{workdir}/"
      FileUtils.mkdir_p "#{workdir}"
      
      file = aws_directory.files.create(
        :key => object_key,
        :body => File.open(object_to_upload),
        :public => false,
        :multipart_chunk_size => 10*1024*1024 # 10 MB
      )
      
      if file.status == 200
        backup_success = true
        object_size = conn.head_object(@bucket, object_key, options = {}).headers["Content-Length"].to_i
        print "Backup completed at #{Time.now()}, successfully uploaded #{object_to_upload} to #{@bucket}: filesize in S3 is #{object_size.to_filesize}\n"
        FileUtils.rm "#{object_to_upload}"
        FileUtils.rm_rf "#{workdir}"
      end
    else
      raise "Unable to dump database#{database}!"
    end
  end
      
  backups = []
  conn.reload
  conn.directories.get(@bucket, prefix: "#{@environment}.#{@app_name}/").files.sort_by{ |hsh| hsh.last_modified }.map { |file| backups << file }
  backups[0...-@keep].each do |object|
    puts "deleting: #{object.key}"
    object.destroy
  end
end